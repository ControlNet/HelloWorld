{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Text Feature Extraction\n",
    "\n",
    "Environment: Python 3.7 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* requests (for downloading pdf files from internet, included in Python 3.7)\n",
    "* os (for os operation, included in Python 3.7)\n",
    "* PyPDF2 (for extract text from pdf, downloaded from pip)\n",
    "* tabula (for extract table from pdf, downloaded from pip)\n",
    "* pandas (for data frame to manipulate data, included in Anaconda 3)\n",
    "* re (for extracting string, included in Python 3.7)\n",
    "* nltk (for English processing, downloaded from pip)\n",
    "* pdfminer (for extract text from pdf, downloaded from pip)\n",
    "* io (for using in function to extract pdf text, included in Python 3.7)\n",
    "* functools (for using `reduce()` to process list, included in Python 3.7)\n",
    "* multiprocessing (for using `Pool` to boost the speed of processing in MacOS and Linux, included in Python 3.7)\n",
    "* types (for using function annotation for better cooperation and readability, incluede in Python 3.7)\n",
    "* platform (for recognizing the operating system of user's, included in Python 3.7)\n",
    "* tqdm(for visualize the processing bar, downloaded from pip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is aim to :\n",
    "\n",
    "Generate a sparse representation for Paper Bodies (i.e. paper text without Title, Authors,\n",
    "Abstract and References). The sparse representation consists of two files:\n",
    "\n",
    "1. Vocabulary index file\n",
    "2. Sparse count vectors file\n",
    "\n",
    "Generate a CSV file (stats.csv) containing three columns:\n",
    "1. Top 10 most frequent terms appearing in all Titles\n",
    "2. Top 10 most frequent Authors\n",
    "3. Top 10 most frequent terms appearing in all Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
      "Building wheels for collected packages: pypdf2\n",
      "  Building wheel for pypdf2 (setup.py): started\n",
      "  Building wheel for pypdf2 (setup.py): finished with status 'done'\n",
      "  Created wheel for pypdf2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61085 sha256=240a1a1ca38aa9760c3f1bdf27d489b8dc92b7a23e5a7496d9faee14b303c91c\n",
      "  Stored in directory: c:\\users\\controlnet\\appdata\\local\\pip\\cache\\wheels\\b1\\1a\\8f\\a4c34be976825a2f7948d0fa40907598d69834f8ab5889de11\n",
      "Successfully built pypdf2\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-1.26.0\n",
      "Collecting tabula-py\n",
      "  Downloading tabula_py-2.3.0-py3-none-any.whl (12.0 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from tabula-py) (1.20.1)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from tabula-py) (1.2.4)\n",
      "Collecting distro\n",
      "  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->tabula-py) (1.15.0)\n",
      "Installing collected packages: distro, tabula-py\n",
      "Successfully installed distro-1.6.0 tabula-py-2.3.0\n",
      "Collecting pdfminer\n",
      "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.11.0-cp35-abi3-win_amd64.whl (1.8 MB)\n",
      "Building wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py): started\n",
      "  Building wheel for pdfminer (setup.py): finished with status 'done'\n",
      "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140117 sha256=425024503d0a3436c42b4c315c4a995e659853baf4378db2bf5a4c310cf29d83\n",
      "  Stored in directory: c:\\users\\controlnet\\appdata\\local\\pip\\cache\\wheels\\1c\\28\\7d\\f390b82bb0307deb63ff27a1474fd308ec68ee028cb9ab6283\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.11.0\n",
      "Requirement already satisfied: nltk in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: chardet in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from pdfminer.six) (4.0.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from pdfminer.six) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\controlnet\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.20)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20211012\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf2\n",
    "!pip install tabula-py\n",
    "!pip install pdfminer\n",
    "!pip install nltk\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import PyPDF2\n",
    "import tabula\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.probability import *\n",
    "# pdfminer below is using for parse pdf file to text\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "from types import FunctionType\n",
    "from tqdm.notebook import tqdm\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\controlnet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tabula is used here for extract table from PDF `(data.pdf)` into DataFrame.</b> (Ariga, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PP3169.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PP3171.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PP3174.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PP3292.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PP3297.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>PP7174.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>PP7185.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>PP7193.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>PP7217.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>PP7249.pdf</td>\n",
       "      <td>https://drive.google.com/uc?export=download&amp;id...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename                                                url\n",
       "0    PP3169.pdf  https://drive.google.com/uc?export=download&id...\n",
       "1    PP3171.pdf  https://drive.google.com/uc?export=download&id...\n",
       "2    PP3174.pdf  https://drive.google.com/uc?export=download&id...\n",
       "3    PP3292.pdf  https://drive.google.com/uc?export=download&id...\n",
       "4    PP3297.pdf  https://drive.google.com/uc?export=download&id...\n",
       "..          ...                                                ...\n",
       "199  PP7174.pdf  https://drive.google.com/uc?export=download&id...\n",
       "200  PP7185.pdf  https://drive.google.com/uc?export=download&id...\n",
       "201  PP7193.pdf  https://drive.google.com/uc?export=download&id...\n",
       "202  PP7217.pdf  https://drive.google.com/uc?export=download&id...\n",
       "203  PP7249.pdf  https://drive.google.com/uc?export=download&id...\n",
       "\n",
       "[204 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the pdf table to a dataframe\n",
    "urls = tabula.read_pdf(\"data.pdf\", output_format=\"dataframe\", pages=\"all\")[0]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>All pages have a title which affect the process to download PDF files, so the filename title shuld be deleted here.<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the data frame as preparation of auto-downloading\n",
    "index_for_drop = urls[urls.filename==\"filename\"].index\n",
    "urls = urls\\\n",
    "    .drop(index_for_drop)\\\n",
    "    .reset_index()\\\n",
    "    .drop([\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Downloading 200 pdf files</b>\n",
    "\n",
    "Please be patient, this chunk will spend some minutes to download 200 pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful.\n"
     ]
    }
   ],
   "source": [
    "def download_files_from_df(df):\n",
    "    # iterate rows in data frame\n",
    "    for row in df.values:\n",
    "        name, url = row\n",
    "        # skip if the file is already downloaded\n",
    "        if os.path.exists(name):\n",
    "            continue\n",
    "        # download the pdf content\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except:\n",
    "            continue\n",
    "        # save as a file\n",
    "        with open(\"pdf/\" + name, \"wb\") as pdf_file:\n",
    "            pdf_file.write(req.content)\n",
    "\n",
    "download_files_from_df(urls)\n",
    "print(\"Download successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting PDF to Text\n",
    "\n",
    "<b>Define the function to extract pdf content into text.</b> (Mike, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_name):\n",
    "    pdf_rm = PDFResourceManager()\n",
    "    io = StringIO()\n",
    "    converter = TextConverter(rsrcmgr=pdf_rm, outfp=io, codec='utf-8', laparams=LAParams())\n",
    "    with open(pdf_name, 'rb') as file:\n",
    "        interpreter = PDFPageInterpreter(pdf_rm, converter)\n",
    "        page_nos = set()\n",
    "        for page in PDFPage.get_pages(file, page_nos, maxpages=0, password=\"\", caching=True,\n",
    "                                      check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "    output_str = io.getvalue()\n",
    "    converter.close()\n",
    "    io.close()\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Using a pdf file as an example to examine the content</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALICE: Towards Understanding Adversarial',\n",
       " 'Learning for Joint Distribution Matching',\n",
       " '',\n",
       " 'Authored by:',\n",
       " '',\n",
       " 'Lawrence Carin',\n",
       " 'Ricardo Henao',\n",
       " 'Changyou Chen',\n",
       " 'Chunyuan Li',\n",
       " 'Yuchen Pu']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_example = extract_pdf_content(\"pdf/PP7133.pdf\")\n",
    "pdf_rows_example = pdf_example.split(\"\\n\")\n",
    "pdf_rows_example[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating functions to get Paper Bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#1 This function is used to delete all hex_codes\n",
    "def delete_hex_code(row):\n",
    "    new_row = row.replace(\"\\x0c\", \"\")\n",
    "    return new_row\n",
    "```\n",
    "=======================================================================\n",
    "```python\n",
    "#2 In this function #3 function will be called to get all content of paper body\n",
    "def extract_body(row_list: list) -> list:\n",
    "    body_begin_index = find_index(r\"1 Paper Body\", row_list)+1\n",
    "    body_end_index = find_index(r\"2 References\", row_list)\n",
    "    return row_list[body_begin_index:body_end_index]\n",
    "\n",
    "#3 identify the index of start and end of paper body.\n",
    "def find_index(regex, a_list):\n",
    "    for index in range(len(a_list)):\n",
    "        if re.search(regex, a_list[index]) is not None:\n",
    "            return index\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>According to above example, there are some hex_code in the content. Following function will delete all hex_codes.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALICE: Towards Understanding Adversarial',\n",
       " 'Learning for Joint Distribution Matching',\n",
       " 'Authored by:',\n",
       " 'Lawrence Carin',\n",
       " 'Ricardo Henao',\n",
       " 'Changyou Chen',\n",
       " 'Chunyuan Li',\n",
       " 'Yuchen Pu',\n",
       " 'Liqun Chen',\n",
       " 'Hao Liu']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "implement the function to delete hex code \"\\x0c\", \n",
    "which is used for split the page in pdfminer parser\n",
    "\"\"\"\n",
    "def delete_hex_code(row):\n",
    "    new_row = row.replace(\"\\x0c\", \"\")\n",
    "    return new_row\n",
    "\n",
    "# split into rows\n",
    "pdf_rows_example = pdf_example.split(\"\\n\")\n",
    "# drop blank rows\n",
    "pdf_rows_example = list(filter(lambda x: x != '', pdf_rows_example))\n",
    "# apply the function to delete hex code\n",
    "pdf_rows_example = list(map(delete_hex_code, pdf_rows_example))\n",
    "pdf_rows_example[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Paper body begin with `1 Paper Bosy` and end with `2 References`, so these two will be used as the regex in the following function.\n",
    "\n",
    "More detail about the relationship between following 2 functions please refer to the explaination at the beginning of 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract body part from row_list\n",
    "def extract_body(row_list: list) -> list:\n",
    "    body_begin_index = find_index(r\"1 Paper Body\", row_list)+1\n",
    "    body_end_index = find_index(r\"2 References\", row_list)\n",
    "    return row_list[body_begin_index:body_end_index]\n",
    "\n",
    "# define the function to get index from given regular expression pattern\n",
    "def find_index(regex, a_list):\n",
    "    for index in range(len(a_list)):\n",
    "        if re.search(regex, a_list[index]) is not None:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep directed generative models are a powerful framework for modeling complex',\n",
       " 'data distributions. Generative Adversarial Networks (GANs) [1] can implicitly',\n",
       " 'learn the data generating distribution; more speciﬁcally, GAN can learn to sam-',\n",
       " 'ple from it. In order to do this, GAN trains a generator to mimic real samples,',\n",
       " 'by learning a mapping from a latent space (where the samples are easily drawn)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement the function\n",
    "pdf_body_rows_example = extract_body(pdf_rows_example)\n",
    "pdf_body_rows_example[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Merge all bodies in pdf files together in a nested list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The #2 function will call the #1 function!\n",
    "#1 function is used to make a list for pdf contents without blank rows and hex_code\n",
    "\n",
    "#2 function is used to extract the body.\n",
    "\n",
    "```python\n",
    "if is_ok_for_pool:\n",
    "    p = Pool()\n",
    "    # apply the function to get body_rows from given file name\n",
    "    body_row_list = list(p.map(filename_to_body_rows, filename_list))\n",
    "else:\n",
    "    body_row_list = list(map(filename_to_body_rows, filename_list))\n",
    "```\n",
    "Using #2 function with map() to get paper body and to get a `2-dimensions list` . There are 200 list in this list. \n",
    "\n",
    "As Windows cannot using `Pool` to boost the process in iPython/Jupyter, Windows machine runs single-thread in this process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1132d5b4f6c74ece83e44f11242c9ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1 this function is used to processing from given pdf file name to row_list\n",
    "def filename_to_rows(filename: str) -> list:\n",
    "    # extract pdf content from given file name\n",
    "    try:\n",
    "        string = extract_pdf_content(\"pdf/\" + filename)\n",
    "    except:\n",
    "        return []\n",
    "    # process pdf content to row_list\n",
    "    row_list = string.split(\"\\n\")\n",
    "    row_list = list(filter(lambda x: x != '', row_list))\n",
    "    row_list = list(map(delete_hex_code, row_list))\n",
    "    return row_list\n",
    "\n",
    "#2 this function is used to process from given file name, and extract body part\n",
    "def filename_to_body_rows(filename: str) -> list:\n",
    "    row_list = filename_to_rows(filename)\n",
    "    body_row = extract_body(row_list)\n",
    "    return body_row\n",
    "\n",
    "# collect the file name from the data frame defined in previous\n",
    "filename_list = [each for each in urls.filename]\n",
    "# determine if the operating system is compatible to the multithreading.\n",
    "# as Windows cannot perform Pool inside the Jupyter Notebook\n",
    "is_ok_for_pool = platform.system() in (\"Darwin\", \"Linux\")\n",
    "\n",
    "# if the operating system is MacOS or Linux, using multithreading to boost the process\n",
    "if is_ok_for_pool:\n",
    "    p = Pool()\n",
    "    # apply the function to get body_rows from given file name\n",
    "    body_row_list = list(p.map(filename_to_body_rows, filename_list))\n",
    "else:\n",
    "    body_row_list = list(map(filename_to_body_rows, tqdm(filename_list)))\n",
    "\n",
    "# filter the blank rows\n",
    "body_row_list = list(filter(lambda x: x != [], body_row_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Convert bodies in pdf files together in one list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to make list flat from nested list. (Python, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100806"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function to make list flat from nested list\n",
    "def to_flat_list(list_1: list, list_2: list) -> list:\n",
    "    return list_1 + list_2\n",
    "\n",
    "# using reduce() to flat list\n",
    "merged_body_rows = reduce(to_flat_list, body_row_list)\n",
    "len(merged_body_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sparse Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following operations are necessary\n",
    "* A. The word tokenization must use the following regular expression, r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"\n",
    "* B. The context-independent and context-dependent (with the threshold set to %95) stop words must be removed from the vocab. The context-independent stop words list (i.e, stopwords_en.txt) provided in the zip file must be used.\n",
    "* C. Unigram tokens should be stemmed using the Porter stemmer. (be careful that stemming performs lower casing by default)\n",
    "* D. Rare tokens (with the threshold set to 3%) must be removed from the vocab.\n",
    "* E. Tokens must be normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. (use sentence segmentation to achieve this)\n",
    "* F. Tokens with the length less than 3 should be removed from the vocab.\n",
    "* G. First 200 meaningful bigrams (i.e., collocations), based on highest total frequency in the corpus, must be extracted and included in your tokenization process. Bigrams should not include context-independent stopwords as part of them and they should be separated using double underscore i.e. “__” (example: “artifical__intelligence”)\n",
    "\n",
    "The order of above operations will be ordered as <b>`EAGBDCF`</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Operation E: lowercase words except the capital tokens appearing in the middle of a sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as a first step, we need to lowercase the beginning words of each sentence. we should divide the contents into sentences. The NLTK data package includes a pre-trained Punkt tokenizer for English ,and this tokenizer divides a text into a list of sentences(nltk, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input.',\n",
       " 'many computational models try to predict such voluntary eye and attentional shifts.',\n",
       " 'although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to transfer the first character of a sentence into lowercase\n",
    "#1\n",
    "def to_lowercase_for_first_word(sentence: str) -> str:\n",
    "    try:\n",
    "        new_sentence = sentence[0].lower()+sentence[1:]\n",
    "    except:\n",
    "        new_sentence = sentence\n",
    "    return new_sentence\n",
    "\n",
    "# there are some words using \"-\" connecting in 2 different rows,\n",
    "# defining the function to merge this type words together\n",
    "#2\n",
    "def join_splited_words(string: str) -> str:\n",
    "    split_pattern = r'(?<=\\w)- (?=\\w+?)'\n",
    "    new_string = re.sub(split_pattern, \"\", string)\n",
    "    return new_string\n",
    "\n",
    "# define the function to segment sentence, and process by calling functions above\n",
    "#3\n",
    "def to_lowercase_as_sentence(body_rows: list) -> list:\n",
    "    sentence_spliter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    body_string = \" \".join(body_rows).strip()\n",
    "    body_string = join_splited_words(body_string)\n",
    "    sentence_list = sentence_spliter.tokenize(body_string)\n",
    "    sentence_list_lower = list(map(to_lowercase_for_first_word, sentence_list))\n",
    "    return sentence_list_lower\n",
    "\n",
    "# apply the function\n",
    "merged_sentences = to_lowercase_as_sentence(merged_body_rows)\n",
    "merged_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Operation A: The word tokenization must use the following regular expression, r\"[A-Za-z]\\w+(?:['?]\\w+)?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, using `nltk.tokenize.RegexpTokenizer` to tokenize words from sentence list with the regular expression provided. (nltk, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799387\n",
      "27991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A0', 'A00', 'A0L', 'A1', 'A1?they', 'A1iEnd', 'A1ij', 'A1ji', 'A2', 'A2S']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to tokenize word \n",
    "def tokenize_word(sentences: list, regex: str) -> list:\n",
    "    tokenizer = RegexpTokenizer(regex)\n",
    "    tokens = tokenizer.tokenize(\" \".join(sentences))\n",
    "    return tokens\n",
    "\n",
    "# apply the function with the regular expression provided\n",
    "merged_word_tokens = tokenize_word(merged_sentences, r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "print(len(merged_word_tokens))\n",
    "unique_words = list(set(merged_word_tokens))\n",
    "unique_words.sort()\n",
    "print(len(unique_words))\n",
    "unique_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Operation G: First 200 meaningful bigrams  (i.e., collocations), based on highest total frequency in the corpus, must be extracted and included in your tokenization process. Bigrams should not include context-independent stopwords as part of them and they should be separated using double underscore i.e. “\\_\\_”  (example: “artifical__intelligence”) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, using `nltk.collocations.BigramAssocMeasures` and `nltk.collocations.BigramCollocationFinder` to find 200 most frequency bigrams from given tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('upper', 'bound'),\n",
       " ('lower', 'bound'),\n",
       " ('training', 'data'),\n",
       " ('neural', 'networks'),\n",
       " ('training', 'set')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to filter the bigrams without stop words\n",
    "def filter_bigrams_meaningful(bigrams):\n",
    "    is_not_included_stopwords = lambda x: not (x[0].lower() in stopwords or x[1].lower() in stopwords)\n",
    "    return list(filter(is_not_included_stopwords, bigrams))\n",
    "\n",
    "# define the function to generate top200 bigrams\n",
    "def generate_top200_bigrams(bigrams):\n",
    "    # recursion\n",
    "    # drop bigrams with stop words\n",
    "    bigrams = filter_bigrams_meaningful(bigrams)\n",
    "    length = len(bigrams)\n",
    "    return bigrams[:200]\n",
    "\n",
    "# get stop words set from \"stopwords_en.txt\" which is provided\n",
    "stopwords = set([line.rstrip('\\n') for line in open(\"stopwords_en.txt\")]) \n",
    "# create the objects needed to implement bigram parser\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(merged_word_tokens)\n",
    "bigram_finder.apply_freq_filter(2)\n",
    "# apply the function in previous\n",
    "top_bigrams = generate_top200_bigrams(bigram_finder.nbest(bigram_measures.raw_freq, int(len(merged_word_tokens)/4)))\n",
    "print(len(top_bigrams))\n",
    "top_bigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the top 200 bigrams, next step is to re-token bigrams in tokens by implementing `nltk.tokenize.MWETokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "787746"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to replace the bigrams in the token list\n",
    "def replace_bigrams(tokens: list, bigrams: list) -> list:\n",
    "    mwetokenizer = MWETokenizer(bigrams, separator='__')\n",
    "    new_tokens = mwetokenizer.tokenize(tokens)\n",
    "    return new_tokens\n",
    "\n",
    "# apply the function\n",
    "merged_word_token_processed = replace_bigrams(merged_word_tokens, top_bigrams)\n",
    "len(merged_word_token_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Operation B: The context-independent and context-dependent (with the threshold set to 95%) stop words must be removed from the vocab. The context-independent stop words list (i.e, stopwords_en.txt) provided in the zip file must be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to remove context independent words, which is provided in `stopwords_en.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['natural', 'viewing', 'conditions', 'human', 'observers']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to remove tokens which is stop words\n",
    "def remove_context_independent_words(tokens: list) -> list:\n",
    "    new_tokens = list(filter(lambda x: x.lower() not in stopwords, tokens))\n",
    "    return new_tokens\n",
    "\n",
    "# drop the tokens which is duplicated, and apply the function\n",
    "word_tokens_processed = remove_context_independent_words(merged_word_token_processed)\n",
    "print(len(word_tokens_processed))\n",
    "word_tokens_processed[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the context dependent words, it requires a statistic information for each word about the number of documents that each word is contained by them. Therefore, to find it, next step is to process the tokens for each pdf files. \n",
    "\n",
    "Please be patient. The code chunk below need few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the function to apply the processing in previous for each pdf file\n",
    "def generate_word_token_from_body_row(body_row: list) -> list:\n",
    "    # process the body rows to sentences\n",
    "    sentences = to_lowercase_as_sentence(body_row)\n",
    "    # process the sentences to word tokens\n",
    "    tokens = tokenize_word(sentences, r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "    # process the tokens with replacing bigrams\n",
    "    tokens = replace_bigrams(tokens, top_bigrams)\n",
    "    # process the tokens to drop stop words\n",
    "    tokens = remove_context_independent_words(tokens)\n",
    "    return tokens\n",
    "\n",
    "# apply the function\n",
    "word_tokens_each_pdf = list(map(generate_word_token_from_body_row, body_row_list))\n",
    "# the list copied is used for count vector\n",
    "word_tokens_each_pdf_list = word_tokens_each_pdf.copy()  \n",
    "# drop the dulicated elements\n",
    "word_tokens_each_pdf = list(map(set, word_tokens_each_pdf))\n",
    "#word_tokens_each_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, after getting the tokens for each pdf files, define the function `merge_set()` and use `reduce()` to merge all set for each tokens. Meanwhile, this function `merge_set()` has a side effect, which can count the number of files that contains the word, and store the infomation in the dictionary `word_dict`.\n",
    "\n",
    "As for `word_dict`, the keys of the dictionary contains all words that in 200 files, and the values of the dictionary contains the number of pdf files containing the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionary to store the information about the frequency\n",
    "word_dict = {}\n",
    "# set the count of words with in first pdf as 1\n",
    "for word in word_tokens_each_pdf[0]:\n",
    "    word_dict[word] = 1\n",
    "# define the function to merge token sets from each pdf,\n",
    "# and using the side effect to get the frequency information\n",
    "def merge_set(word_set1: set, word_set2: set):\n",
    "    # if the word appears in previous, increment the count by 1\n",
    "    inter_set = word_set2.intersection(word_set1)\n",
    "    for word in inter_set:\n",
    "        word_dict[word] += 1\n",
    "    # if the word does not appear in previous, set the count to 1\n",
    "    difference_set = word_set2.difference(word_set1)\n",
    "    for word in difference_set:\n",
    "        word_dict[word] = 1\n",
    "    # return the merged set\n",
    "    return word_set2.union(word_set1)\n",
    "    \n",
    "# apply the function by reduce()\n",
    "word_tokens_union = reduce(merge_set, word_tokens_each_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Operation D: Rare tokens (with the threshold set to 3%) must be removed from the vocab.\n",
    "\n",
    "After getting the information in the previous step, using the keys in `word_dict` for each word to judge if it should be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4698"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to delete countext dependent words and rare words\n",
    "def remove_context_dependent_words_and_rare_words(tokens: set) -> set:\n",
    "    delete_list = []\n",
    "    pdf_amount = len(filename_list)\n",
    "    for key, value in word_dict.items():\n",
    "        if value >= pdf_amount*0.95 or value < pdf_amount*0.03:\n",
    "            delete_list.append(key)\n",
    "    new_tokens = set(filter(lambda x: x not in delete_list, tokens))\n",
    "    return new_tokens\n",
    "\n",
    "# apply the function\n",
    "word_tokens_union_processed = remove_context_dependent_words_and_rare_words(word_tokens_union)\n",
    "len(word_tokens_union_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Operation C: Unigram tokens should be stemmed using the Porter stemmer. \n",
    "Then, using `nltk.stem.PorterStemmer` to stem the word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A1', 'AB', 'AP', 'ARO', 'AUC']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the to_stem function to process the words which is lowercase\n",
    "def to_stem(word: str) -> str:\n",
    "    if word[0].isupper():\n",
    "        new_word = word\n",
    "    elif word.count(\"_\") > 0:\n",
    "        new_word = word\n",
    "    else:\n",
    "        new_word = stemmer.stem(word)\n",
    "    return new_word\n",
    "\n",
    "# initialize the PorterStemmer, and apply the function\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.mode = PorterStemmer.NLTK_EXTENSIONS\n",
    "final_tokens = list(map(to_stem, word_tokens_union_processed))\n",
    "# delete the duplicate tokens and sort\n",
    "final_tokens = list(set(final_tokens))\n",
    "final_tokens.sort()\n",
    "# display\n",
    "print(len(final_tokens))\n",
    "final_tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Operation F: Tokens with the length less than 3 should be removed from the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, next step is to delete short words whose length is less than 3, by defining the `delete_short_words` function and applying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ARO', 'AUC', 'Accuracy', 'Acknowledgements', 'Acknowledgments']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to delete the short words with the length < 3\n",
    "def delete_short_words(tokens: list) -> list:\n",
    "    is_not_short = lambda x: len(x) >= 3\n",
    "    new_tokens = list(filter(is_not_short, tokens))\n",
    "    return new_tokens\n",
    "\n",
    "# apply the function, and delete duplicated tokens\n",
    "final_tokens_fixed = list(delete_short_words(final_tokens))\n",
    "print(len(final_tokens_fixed))\n",
    "final_tokens_fixed[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Output vocabulary index file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting each necessary data from the bodies of 200 pdf files, next step is to output the files required.\n",
    "\n",
    "To output vocabulary index file, firstly combine the index number to each words, which are sorted by alphabetical ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ARO', 0),\n",
       " ('AUC', 1),\n",
       " ('Accuracy', 2),\n",
       " ('Acknowledgements', 3),\n",
       " ('Acknowledgments', 4)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the tokens, and assign each words with indexes\n",
    "final_tokens_fixed.sort()\n",
    "vocabulary_index_pairs = list(zip(final_tokens_fixed, list(range(len(final_tokens_fixed)))))\n",
    "vocabulary_index_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word, generate a string representing a row, and use `reduce()` to process all words in `vocabulary_index_pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARO:0\\nAUC:1\\nAccuracy:2\\nAcknowledgements:3\\nAcknowledgments:4\\nAdam:5\\nAdaptive:6\\nAij:7\\nAlgorithm:8\\nAlgo'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to generate each row in output file,\n",
    "# and merge them together by reduce() function\n",
    "def generate_vocabulary_index(formal_text: str, pair: tuple) -> str:\n",
    "    row = pair[0] + \":\" + str(pair[1])\n",
    "    return formal_text + \"\\n\" + row\n",
    "\n",
    "# apply the function with reduce()\n",
    "vocabulary_index_text = reduce(generate_vocabulary_index, [\"\"]+vocabulary_index_pairs).lstrip(\"\\n\")\n",
    "vocabulary_index_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, write the file by using `file.write()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the file as output\n",
    "with open(\"vocab.txt\", \"w\", encoding=\"UTF-8\") as output_file:\n",
    "    output_file.write(vocabulary_index_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Generate sparse count vectors file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate sparse count vectors file, for each word, the number of pdf files containing the word should be collected. Although, the information has been collected in 4.4 Operation B, in this section, the word tokens should be stemmed. Therefore, the stem processed should be applied in `word_tokens_each_pdf_list` which is copied from the `word_tokens_each_pdf` in 4.4 Operation B.\n",
    "\n",
    "So, define a high-order function `apply_func()` to generate a function for map the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9a6ee7901a41858af0fc72f56ae39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a function to generate function for map inside the list\n",
    "def apply_func(func: FunctionType) -> FunctionType:\n",
    "    def map_list(x):\n",
    "        return list(map(func, x))\n",
    "    return map_list\n",
    "\n",
    "# apply the function\n",
    "apply_to_stem = apply_func(to_stem)\n",
    "stemmed_tokens_each_pdf_list = list(map(apply_to_stem, tqdm(word_tokens_each_pdf_list)))\n",
    "# stemmed_tokens_each_pdf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the words in the keys of `vocabulary_index_pairs` with the word tokens for each pdf files will get the necessary data of sparse count vectors. Then, store the data for a pdf file for each words as strings that are required, and store the count vectors in a list for each pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f488c114cf7d4a718a70e933446f9ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the list to store count vector\n",
    "count_vectors = []\n",
    "pdf_names = list(map(lambda x: x.replace(\".pdf\", \"\"), urls.filename))\n",
    "\n",
    "# iterate each pdf tokens\n",
    "for pdf_index in tqdm(range(len(stemmed_tokens_each_pdf_list))):\n",
    "    # initialize the count_vector string with the file name\n",
    "    count_vector = pdf_names[pdf_index] + \",\"\n",
    "    pdf_tokens = stemmed_tokens_each_pdf_list[pdf_index]\n",
    "    \n",
    "    # iterate each token to append the string\n",
    "    for word, word_index in vocabulary_index_pairs:\n",
    "        count = pdf_tokens.count(word)\n",
    "        if count == 0:\n",
    "            # if this word does not exist in the document, skip\n",
    "            continue\n",
    "        else:\n",
    "            # append the string\n",
    "            count_vector += (str(word_index)+\":\"+str(count)+\",\")\n",
    "    # delete the last useless \",\"\n",
    "    count_vector = count_vector.rstrip(\",\")\n",
    "    count_vectors.append(count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the count vector list together with `\\n` to ensure the vectors for each pdf files locating in different rows to merge the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PP3169,1:7,57:1,58:1,64:1,96:1,97:1,118:1,121:2,131:8,132:2,161:1,196:1,198:1,251:1,272:1,296:1,327:'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the count vectors for each pdf into a string\n",
    "count_vector_text = \"\\n\".join(count_vectors)\n",
    "count_vector_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to output the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output file\n",
    "with open(\"count_vectors.txt\", \"w\", encoding=\"UTF-8\") as output_file:\n",
    "    output_file.write(count_vector_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Statistics Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Merge all Titles, Authors and Abstracts into 3 lists\n",
    "#### 5.1.1 Function for extracting title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to extract title from row_list\n",
    "def extract_title(row_list: list) -> str:\n",
    "    end_index = find_index(r\"Authored by:\", row_list)\n",
    "    title_rows = row_list[:end_index]\n",
    "    title_string = \" \".join(title_rows)\n",
    "    return title_string\n",
    "\n",
    "extract_title(pdf_rows_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2  Function for extracting authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lawrence Carin',\n",
       " 'Ricardo Henao',\n",
       " 'Changyou Chen',\n",
       " 'Chunyuan Li',\n",
       " 'Yuchen Pu',\n",
       " 'Liqun Chen',\n",
       " 'Hao Liu']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to extract authors from row_list\n",
    "def extract_authors(row_list: list) -> list:\n",
    "    start_index = find_index(r\"Authored by:\", row_list) + 1\n",
    "    end_index = find_index(r\"Abstract\", row_list)\n",
    "    author_list = row_list[start_index: end_index]\n",
    "    abstract_text_total = \"\".join(author_list)\n",
    "    return author_list\n",
    "\n",
    "extract_authors(pdf_rows_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3  Function for extracting abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we investigate the non-identiﬁability issues associated with bidirectional adversarial training for '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to extract abstract from row_list,\n",
    "# and process to lowercase which is needed\n",
    "def processing_abstract(abstract_rows: list) -> str:\n",
    "    sentence = to_lowercase_as_sentence(abstract_rows)\n",
    "    return sentence\n",
    "\n",
    "def extract_abstract(row_list: list) -> str:\n",
    "    start_index = find_index(r\"Abstract\", row_list) + 1\n",
    "    end_index = find_index(r\"1 Paper Body\", row_list)\n",
    "    abstract_rows = row_list[start_index: end_index]\n",
    "    sentence = processing_abstract(abstract_rows)\n",
    "    abstract_text_total = \" \".join(sentence)\n",
    "    return abstract_text_total\n",
    "\n",
    "extract_abstract(pdf_rows_example)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4 Get pdf contents and implement the functions (get 3 lists)\n",
    "This chunk will require several minutes to process 200 pdf files. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ada13f0e7184b30ac786bb46daf0626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the function which generate the rows of title, author and abstract seperately\n",
    "def filename_to_extract_rows(filename: str) -> list:\n",
    "    try:\n",
    "        string = extract_pdf_content(\"pdf/\" + filename)\n",
    "    except:\n",
    "        return []\n",
    "    row_list = string.split(\"\\n\")\n",
    "    row_list = list(filter(lambda x: x != '', row_list))\n",
    "    title_row = extract_title(row_list)\n",
    "    author_row = extract_authors(row_list)\n",
    "    abstract_row = extract_abstract(row_list)\n",
    "    return (title_row, author_row, abstract_row)\n",
    "\n",
    "# apply the function\n",
    "# if the operating system is MacOS or Linux, using multithreading to boost the process\n",
    "if is_ok_for_pool:\n",
    "    p2= Pool()\n",
    "    row_lists = list(p2.map(filename_to_extract_rows, filename_list))\n",
    "else:\n",
    "    row_lists = list(map(filename_to_extract_rows, tqdm(filename_list)))\n",
    "\n",
    "# store each part to each variables for future usage\n",
    "title_row_list = [each[0] for each in row_lists]\n",
    "author_row_list = [each[1] for each in row_lists]\n",
    "abstract_row_list = [each[2] for each in row_lists]\n",
    "# delete the blank rows in title\n",
    "title_row_list = list(filter(lambda x: x != [], title_row_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5  Function for lowercasing titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predicting human gaze using low-level saliency combined with face detection',\n",
       " 'mining internet-scale software repositories',\n",
       " 'an online hebbian learning rule that performs independent component analysis',\n",
       " 'fast variational inference for large-scale internet diagnosis',\n",
       " 'receding horizon diﬀerential dynamic programming',\n",
       " 'a conﬁgurable analog vlsi neural network with spiking neurons and self-regulating plastic synapses',\n",
       " 'robust regression with twinned gaussian processes',\n",
       " 'partially observed maximum entropy discrimination markov networks',\n",
       " 'global ranking using continuous conditional random fields',\n",
       " 'an empirical analysis of domain adaptation algorithms for genomic sequence analysis']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to transfer the title to lowercase\n",
    "def to_lowercase_title(title_rows: list) -> list:\n",
    "    title_string_list = list(map(lambda x: x.lower(), title_rows))\n",
    "    return title_string_list\n",
    "\n",
    "# apply the function\n",
    "merged_lower_title_list = to_lowercase_title(title_row_list)\n",
    "merged_lower_title_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.6 Title words toknization \n",
    "Using tokenize_word( ) function which is defined in 4.2 to get the tokoens\n",
    "```python\n",
    "def tokenize_word(sentences: list, regex: str) -> list:\n",
    "    tokenizer = RegexpTokenizer(regex)\n",
    "    tokens = tokenizer.tokenize(\" \".join(sentences))\n",
    "    return tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446\n",
      "['predicting', 'human', 'gaze', 'using', 'low-level', 'saliency', 'combined', 'with', 'face', 'detection']\n"
     ]
    }
   ],
   "source": [
    "# get word tokens\n",
    "title_tokens = tokenize_word(merged_lower_title_list, r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "print(len(title_tokens))\n",
    "print(title_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove stop words in title\n",
    "In order to remove context-independent stop words the remove_context_independent_words( ) which was defined in <b>4.4</b> will be used here.\n",
    "```python\n",
    "def remove_context_independent_words(tokens: list) -> list:\n",
    "    new_tokens = list(filter(lambda x: x.lower() not in stopwords, tokens))\n",
    "    return new_tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n",
      "['predicting', 'human', 'gaze', 'low-level', 'saliency', 'combined', 'face', 'detection', 'mining', 'internet-scale']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words\n",
    "title_tokens_processed = remove_context_independent_words(title_tokens)\n",
    "print(len(title_tokens_processed))\n",
    "print(title_tokens_processed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tokens frequency information\n",
    "Counting the frequency of each token by using FreqDist( ) which is from the nltk(nltk.tokenize package — NLTK 3.4.5 documentation\", 2019). This will record the number of time each token has occured.\n",
    "\n",
    "It is hard to get the content which is processed by the FreqDist( ). Therefore, we use most_common cuntion to convert it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('learning', 40), ('networks', 18), ('inference', 16), ('analysis', 14), ('neural', 12), ('models', 12), ('deep', 11), ('eﬃcient', 10), ('gaussian', 8), ('bayesian', 8), ('approach', 8), ('data', 8), ('online', 7), ('functions', 7), ('optimization', 7)]\n"
     ]
    }
   ],
   "source": [
    "# get the frequency distribution for each words\n",
    "title_top10_freq = FreqDist(title_tokens_processed)\n",
    "title_top10_freq = title_top10_freq.most_common()\n",
    "print(title_top10_freq[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of ties in any of the above fields, settle the tie based on alphabetical ascending order.\n",
    "The following functions can help to generate the top 10 terms which is ordered firstly by the frequency descending order then ordered by alphabetical ascending order.\n",
    "```python\n",
    "pairs = sort_2d_list_by_2keys(pairs,1,True,0,False)\n",
    "```\n",
    "Firstly,  #3 function order the index1 by descending then order the index0 by alphabetical ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'networks',\n",
       " 'inference',\n",
       " 'analysis',\n",
       " 'models',\n",
       " 'neural',\n",
       " 'deep',\n",
       " 'eﬃcient',\n",
       " 'approach',\n",
       " 'bayesian']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to generate function to extract element in given indexes,\n",
    "# for using in list.sort()\n",
    "def get_element_extractor(*index_tuple: tuple) -> FunctionType:\n",
    "    def extract_elements(a_list):\n",
    "        new_list = []\n",
    "        for index in index_tuple:\n",
    "            new_list.append(a_list[index])\n",
    "        return new_list\n",
    "    return extract_elements\n",
    "\n",
    "# define the function to sort a pair list by given 2 indexes, \n",
    "# and with if it is reversed or not for each index\n",
    "def sort_2d_list_by_2keys(\n",
    "    a_list: list, \n",
    "    index1: int, \n",
    "    reverse1: bool, \n",
    "    index2: int, \n",
    "    reverse2: bool\n",
    ") -> list:\n",
    "    new_list = a_list.copy()\n",
    "    # sort by index1\n",
    "    get_index1 = get_element_extractor(index1)\n",
    "    new_list.sort(key=get_index1, reverse=reverse1)\n",
    "    # extract unique index1 elements for second sort\n",
    "    elements_in_index1 = list(set([x[index1] for x in new_list]))\n",
    "    elements_in_index1.sort(reverse=reverse1)\n",
    "    # sort by index2\n",
    "    result_list = []\n",
    "    for element in elements_in_index1:\n",
    "        sub_list = list(filter(lambda x: x[index1] == element, new_list))\n",
    "        get_index2 = get_element_extractor(index2)\n",
    "        sub_list.sort(key=get_index2, reverse=reverse2)\n",
    "        result_list += sub_list\n",
    "    return result_list\n",
    "       \n",
    "# define the function to get the top 10 terms\n",
    "def get_top10_terms(top10_freq):\n",
    "    # determine the count in rank 10 term\n",
    "    top10_threshold = top10_freq[9][1]\n",
    "    # collect the terms that achieves the count larger or equals to the top 10\n",
    "    pairs = list(filter(lambda x: x[1] >= top10_threshold, top10_freq))\n",
    "    # sort the list by the count in descending, and with the alphabetical ascending order\n",
    "    pairs = sort_2d_list_by_2keys(pairs,1,True,0,False)\n",
    "    # get the top 10\n",
    "    top10_pairs = pairs[:10]\n",
    "    # get the words themselves\n",
    "    top10_terms = list(map(lambda x: x[0], top10_pairs))\n",
    "    return top10_terms\n",
    "\n",
    "# apply the function\n",
    "top10_title_terms = get_top10_terms(title_top10_freq)\n",
    "top10_title_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.7 Merge all pdfs' Authors into a nested list and get top 10 authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Christof Koch', 'Jonathan Harel', 'Moran Cerf', 'Wolfgang Einhaeuser'],\n",
       " ['Pierre F. Baldi',\n",
       "  'Erik Linstead',\n",
       "  'Paul Rigor',\n",
       "  'Sushil Bajracharya',\n",
       "  'Cristina Lopes'],\n",
       " ['Wulfram Gerstner', 'Andr? Longtin', 'Claudia Clopath']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the author list for each pdf\n",
    "author_row_list = list(filter(lambda x: x != [], author_row_list))\n",
    "author_row_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Christof Koch', 'Jonathan Harel', 'Moran Cerf', 'Wolfgang Einhaeuser', 'Pierre F. Baldi']\n"
     ]
    }
   ],
   "source": [
    "# merge the author list by reduce()\n",
    "merged_author_rows = reduce(to_flat_list, author_row_list)\n",
    "print(merged_author_rows[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the frequency of each author by using `FreqDist` which is from the nltk(NLTK, 2019). This will record the number of time each author has occured.\n",
    "\n",
    "It is hard to get the content which is processed by the `FreqDist`. Therefore, we use most_common cuntion to convert it into a list.\n",
    "\n",
    "Finally, using the same function we to get the top 10 most frequent authors:\n",
    "\n",
    "```python\n",
    "def get_top10_terms(top10_freq):\n",
    "    # determine the count in rank 10 term\n",
    "    top10_threshold = top10_freq[9][1]\n",
    "    # collect the terms that achieves the count larger or equals to the top 10\n",
    "    pairs = list(filter(lambda x: x[1] >= top10_threshold, top10_freq))\n",
    "    # sort the list by the count in descending, and with the alphabetical ascending order\n",
    "    pairs = sort_2d_list_by_2keys(pairs,1,True,0,False)\n",
    "    # get the top 10\n",
    "    top10_pairs = pairs[:10]\n",
    "    # get the words themselves\n",
    "    top10_terms = list(map(lambda x: x[0], top10_pairs))\n",
    "    return top10_terms\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Danilo Jimenez Rezende', 'Elad Hazan', 'Kenji Fukumizu', 'Lawrence Carin', 'Sergey Levine', 'Alex J. Smola', 'Andrew Y. Ng', 'Andrew Zisserman', 'Arthur Gretton', 'Aviv Tamar']\n"
     ]
    }
   ],
   "source": [
    "# get the top10 of author names by similar method\n",
    "author_top10_freq = FreqDist(merged_author_rows)\n",
    "author_top10_freq = author_top10_freq.most_common()\n",
    "top10_authors = get_top10_terms(author_top10_freq)\n",
    "print(top10_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.8  Merge all Abstracts in pdf files together and get top 10 tokens in abstract\n",
    "The method is same as above section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the abstract rows\n",
    "abstract_row_list = list(filter(lambda x: x != [], abstract_row_list))\n",
    "# abstract_row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'viewing',\n",
       " 'conditions',\n",
       " 'human',\n",
       " 'observers',\n",
       " 'shift',\n",
       " 'gaze',\n",
       " 'allocate',\n",
       " 'processing',\n",
       " 'resources']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the tokens in abstract, and remove stop words\n",
    "abstract_tokens = tokenize_word(abstract_row_list, r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "abstract_tokens = remove_context_independent_words(abstract_tokens)\n",
    "abstract_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'model',\n",
       " 'learning',\n",
       " 'algorithm',\n",
       " 'problem',\n",
       " 'models',\n",
       " 'show',\n",
       " 'algorithms',\n",
       " 'method',\n",
       " 'approach']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top10 tokens in abstract\n",
    "abstract_top10_freq = FreqDist(abstract_tokens).most_common()\n",
    "top10_abstract_terms = get_top10_terms(abstract_top10_freq)\n",
    "top10_abstract_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Generating CSV file\n",
    "### 5.2.1 create a data frame to store the information about top 10 terms\n",
    "* First of all, creating a DataFrame is necessary to be prepared before outputing csv format by using Pandas.\n",
    "* From 5.1.6, 5.1.7 and 5.1.8, we have generate 3 lists which contain top 10 frequency terms.\n",
    "* Creating a dictionary for dataframe(Pandas, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_terms_in_abstracts</th>\n",
       "      <th>top10_terms_in_titles</th>\n",
       "      <th>top10_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>learning</td>\n",
       "      <td>Danilo Jimenez Rezende</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model</td>\n",
       "      <td>networks</td>\n",
       "      <td>Elad Hazan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning</td>\n",
       "      <td>inference</td>\n",
       "      <td>Kenji Fukumizu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>analysis</td>\n",
       "      <td>Lawrence Carin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>problem</td>\n",
       "      <td>models</td>\n",
       "      <td>Sergey Levine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>models</td>\n",
       "      <td>neural</td>\n",
       "      <td>Alex J. Smola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>show</td>\n",
       "      <td>deep</td>\n",
       "      <td>Andrew Y. Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>eﬃcient</td>\n",
       "      <td>Andrew Zisserman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>method</td>\n",
       "      <td>approach</td>\n",
       "      <td>Arthur Gretton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>approach</td>\n",
       "      <td>bayesian</td>\n",
       "      <td>Aviv Tamar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_terms_in_abstracts top10_terms_in_titles           top10_authors\n",
       "0                     data              learning  Danilo Jimenez Rezende\n",
       "1                    model              networks              Elad Hazan\n",
       "2                 learning             inference          Kenji Fukumizu\n",
       "3                algorithm              analysis          Lawrence Carin\n",
       "4                  problem                models           Sergey Levine\n",
       "5                   models                neural           Alex J. Smola\n",
       "6                     show                  deep            Andrew Y. Ng\n",
       "7               algorithms               eﬃcient        Andrew Zisserman\n",
       "8                   method              approach          Arthur Gretton\n",
       "9                 approach              bayesian              Aviv Tamar"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data frame to store the information about top 10\n",
    "df = pd.DataFrame({\n",
    "    \"top10_terms_in_abstracts\": top10_abstract_terms,\n",
    "    \"top10_terms_in_titles\": top10_title_terms,\n",
    "    \"top10_authors\": top10_authors\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the dataframe created need exported in to a `.csv` files without index, and save as `stats.csv`. Using the to_csv( ) [function](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv) is a approprite way to export the file (Pandas, 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv file\n",
    "df.to_csv(\"stats.csv\", index=None, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the output file is readable by computer, using `pandas.read_csv()` to read the file which is output in previous to check its correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_terms_in_abstracts</th>\n",
       "      <th>top10_terms_in_titles</th>\n",
       "      <th>top10_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>learning</td>\n",
       "      <td>Danilo Jimenez Rezende</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model</td>\n",
       "      <td>networks</td>\n",
       "      <td>Elad Hazan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning</td>\n",
       "      <td>inference</td>\n",
       "      <td>Kenji Fukumizu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>analysis</td>\n",
       "      <td>Lawrence Carin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>problem</td>\n",
       "      <td>models</td>\n",
       "      <td>Sergey Levine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>models</td>\n",
       "      <td>neural</td>\n",
       "      <td>Alex J. Smola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>show</td>\n",
       "      <td>deep</td>\n",
       "      <td>Andrew Y. Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>eﬃcient</td>\n",
       "      <td>Andrew Zisserman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>method</td>\n",
       "      <td>approach</td>\n",
       "      <td>Arthur Gretton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>approach</td>\n",
       "      <td>bayesian</td>\n",
       "      <td>Aviv Tamar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_terms_in_abstracts top10_terms_in_titles           top10_authors\n",
       "0                     data              learning  Danilo Jimenez Rezende\n",
       "1                    model              networks              Elad Hazan\n",
       "2                 learning             inference          Kenji Fukumizu\n",
       "3                algorithm              analysis          Lawrence Carin\n",
       "4                  problem                models           Sergey Levine\n",
       "5                   models                neural           Alex J. Smola\n",
       "6                     show                  deep            Andrew Y. Ng\n",
       "7               algorithms               eﬃcient        Andrew Zisserman\n",
       "8                   method              approach          Arthur Gretton\n",
       "9                 approach              bayesian              Aviv Tamar"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the csv file is readable for computer\n",
    "pd.read_csv(\"stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ariga, A. (2019). tabula-py: Extract table from PDF into Python DataFrame. Retrieved from https://blog.chezo.uno/tabula-py-extract-table-from-pdf-into-python-dataframe-6c7acfa5f302\n",
    "2. Mike. (2019). Exporting Data from PDFs with Python - The Mouse Vs. The Python. Retrieved from http://www.blog.pythonlibrary.org/2018/05/03/exporting-data-from-pdfs-with-python/#targetText=Extracting%20Text%20with%20PDFMiner,as%20father%20information%20about%20fonts\n",
    "3. nltk. (2019). nltk.tokenize package — NLTK 3.4.5 documentation.  Retrieved from https://www.nltk.org/api/nltk.tokenize.html?highlight=english%20pickle\n",
    "4. nltk. (2019). nltk.tokenize.regexp — NLTK 3.4.5 documentation. Retrieved from https://www.nltk.org/_modules/nltk/tokenize/regexp.html\n",
    "5. Pandas. (2019). pandas.DataFrame — pandas 0.25.1 documentation. Retrieved from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "6. Pandas. (2019). pandas.read_csv — pandas 0.25.1 documentation. Retrieved from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "7. Python. (2019). 9.8. functools — Higher order functions and operations on callable objects — Python v3.1.5 documentation. Retrieved from https://docs.python.org/3.1/library/functools.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
